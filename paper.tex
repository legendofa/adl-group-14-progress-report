%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Memory-Augmented Agentic LLM Using Knowledge Graphs and Vector 
Databases for Context-Aware Reasoning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Kieslinger, Julian Felix 2025-82736}{yyy}
\icmlauthor{Salah-Eddine Nifa 2025-82914}{yyy}
\icmlauthor{Jonghyun Shin 2024-30227}{zzz}
\icmlauthor{Wun Hur 2023-26041}{xxx}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Data Science, Seoul National University, Seoul, SK}
\icmlaffiliation{zzz}{Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea}
\icmlaffiliation{xxx}{Interdisciplinary Program in Artificial Intelligence, Seoul National University, Seoul, South Korea}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
This document serves as a progress report for our project titled Memory-Augmented Agentic LLM Using Knowledge Graphs and Vector Databases for Context-Aware Reasoning.
It summarizes the problem being addressed, related work in the area, methodology developed so far, 
current progress, and planned next steps.
\end{abstract}

\section{Problem Definition}
Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating natural language. However, their effectiveness is inherently limited by the fixed-size context window, which restricts their ability to utilize long-term, evolving knowledge during extended interactions. Traditional LLMs lack persistent memory mechanisms to store, retrieve, and update information accumulated over time, leading to challenges in maintaining contextual consistency, adapting to corrections, and providing coherent responses over long-term dialogues or complex tasks.

Current approaches to augment LLMs with memory primarily rely either on unstructured retrieval from static document stores or on isolated vector embeddings, both of which suffer from limited interpretability and insufficient integration between structured and unstructured knowledge. This insufficiency hampers the model’s ability to perform context-aware reasoning, dynamic knowledge updating, and sophisticated planning, particularly in scenarios demanding continuous learning and adaptation.

The core problem addressed in this work is the design of a hybrid memory-augmented, agentic LLM architecture capable of persistent knowledge accumulation, dynamic refinement, and contextually relevant reasoning across long-term interactions. This system must effectively integrate structured memory—using knowledge graphs to represent explicit entities and their relationships—with unstructured semantic memory—using vector databases to enable flexible, similarity-based retrieval. The agent must also autonomously update its internal world model, incorporating user feedback and evolving contextual cues, thereby bridging the gap between limited short-term language context and the necessity for a robust, constantly evolving long-term memory.

Moreover, such an architecture must enable efficient scaling to real-world application complexities, balancing computational resource constraints with the need for accurate and contextually grounded reasoning. To quantitatively evaluate the system’s capabilities, benchmarking against comprehensive datasets such as LongMemEval will provide transparent, verifiable results demonstrating improvements in memory consistency, relevance, and adaptability.

By addressing these challenges, the proposed framework aims to push the boundaries of current LLM capabilities, advancing toward fully autonomous agents capable of sustained, context-aware reasoning and decision-making in diverse real-world domains such as software development assistance, personal knowledge management, and interactive dialogue systems.

\section{Related Works}

Large Language Models (LLMs) have demonstrated remarkable reasoning and generative abilities, yet they remain constrained by a fixed-size context window, which limits their ability to recall information across extended dialogues. Early dialogue-based studies highlighted this ``goldfish memory'' problem, where LLMs rapidly lose track of prior context across long interactions \cite{openai_memory2024}. Simple prompt-history concatenation or full-context attention models partially mitigate this issue but incur quadratic cost growth and lack long-term adaptability.

To overcome these limitations, retrieval-augmented generation (RAG) methods incorporate external knowledge stores that supply relevant context dynamically. Vector databases such as \textit{pgvector} and \textit{Pinecone} enable efficient semantic retrieval through embedding similarity search \cite{pgvector2023}. While this unstructured memory improves flexibility, it often suffers from noisy retrieval and limited interpretability. In contrast, structured representations like \textbf{knowledge graphs} offer explicit relational reasoning capabilities. 

Recent hybrid approaches combine these two paradigms. Microsoft’s \textbf{GraphRAG} constructs entity-centric graphs from large corpora and employs LLM-generated community summaries to improve retrieval quality over pure vector-based RAG \cite{graphrag2024}. However, its static graph construction leads to slow updates and high query latency, making it less suitable for dynamic knowledge streams. The open-source system \textbf{Graphiti} extends this concept by maintaining a temporally evolving graph database (Neo4j) that incrementally updates entity relations, blending semantic vector search with graph traversal for more contextually grounded responses \cite{graphiti2025}. Empirical results on conversational benchmarks demonstrate improved factual consistency and memory persistence compared to vanilla RAG.

Another complementary research direction gives LLMs explicit control over their own memory. \textbf{MemGPT} introduced an operating-system–inspired virtual memory manager that partitions information into “main” and “external” memory and allows the model to swap relevant context dynamically during conversation \cite{memgpt2023}. This enables scalable long-context reasoning without exceeding token limits. Similarly, persistent-memory frameworks in OpenAI’s \textbf{ChatGPT Memory Update (2024)} \cite{openai_memory2024} store user-specific information across sessions, marking a commercial step toward continuous, personalized agents.

To evaluate such systems, benchmarks like \textbf{LongMemEval} \cite{longmemeval2024} were proposed to assess an agent’s long-term factual retention, consistency, and adaptability over multi-session dialogues averaging more than 100K tokens of cumulative context. Systems employing hybrid knowledge–semantic memory, such as Graphiti, have achieved higher relevance and accuracy scores on LongMemEval by retrieving structured facts rather than full histories.

In summary, existing approaches—including GraphRAG, Graphiti, MemGPT, and persistent OpenAI memory—have significantly advanced LLM memory architectures. Yet none fully integrate interpretable structured knowledge with scalable unstructured semantic recall under autonomous update control. Our proposed framework bridges this gap by combining a \textbf{Knowledge Graph} for explicit entity relations with a \textbf{Vector Database} for flexible semantic recall, enabling a persistent, adaptive, and context-aware memory foundation for next-generation agentic LLMs.

\section{Methodology}

In this section, we will explore how our approach systematically improves and integrates parts from existing methods and how we evaluate our model against the state-of-the-art.

\subsection{Experimental configuration}

Our main tool of evaluation is the \textbf{LongMemEval} benchmark \cite{longmemeval2024}, which guides our system development and testing. In order to evaluate our approach, we first decide on our two baseline models:

\begin{enumerate}
    \item \textbf{GPT-4o} via OpenAI API: This model serves as a strong baseline due to its advanced capabilities and wide adoption. We use it in a zero-shot setting to evaluate its inherent long-term memory capabilities without any augmentation.
    \item \textbf{Zep}, a memory agent built upon Graphiti \cite{graphiti2025}: As an open-source system that combines knowledge graphs with vector databases, Graphiti provides the core for our own Zep baseline implementation. Zep is not open source, but we implement our own version of Zep, based on the descriptions in the paper.
\end{enumerate}

\subsection{Zep Baseline Construction}

In order to remodel Zep, we first need to understand which components Zep consists of. Therefore, we conceptualized the Zep extraction workflow, which describes how Zep extracts information from user inputs and stores it in its memory systems. Figure \ref{fig:zep-extraction-workflow} illustrates the Zep extraction workflow. It consists of three sub-workflows:

\begin{enumerate}
    \item \textbf{Episode Creation}, where an input is linked to existing episodes via the bi-temporal representation that the authors propose in the Zep paper. \cite{graphiti2025}. The system extracts the relative timestamp through temporal normalization and adds the corresponding nodes and edges in the knowledge graph.
    \item \textbf{Entity Creation}, where the system extracts the relative entities from the input and links them to existing entities in the knowledge graph or creates new entities if they do not exist yet.
    \item \textbf{Fact Creation}, where the system extracts atomic facts from the input and links them to the corresponding entities and episodes in the knowledge graph. Facts are also embedded and stored in the knowledge graph.
\end{enumerate}

An additional concept that Zep introduces is community nodes. The authors acknowledge that the creation of community nodes through incremental updates leads to divergence over time; the authors instead propose a periodic job which creates and updates the community nodes.

For our implementation of Zep, we make use of the Graphit MCP server implementation, which allows us to add user input to the knowledge graph \cite{graphitirepo2025}. We utilize the \texttt{add\_episode} endpoint, which implements episode creation, entity extraction, and fact extraction in itself. The user input is sent to the MCP server, which extracts the relevant information and adds it to the knowledge graph accordingly.
The MCP server allows us then multiple search tools for retrieving information from the knowledge graph: \texttt{search\_nodes}, \texttt{search\_facts}, \texttt{get\_episodes}. We aim to repopulate the context of the agent by utilizing these search tools.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{img/zep-extraction-workflow.png}
    \caption{Zep Extraction Workflow}
    \label{fig:zep-extraction-workflow}
\end{figure*}

For simplification purposes, we opt for leaving out the community node creation for now, as it is not essential for the functioning of Zep. Our results should be transitive, as the community nodes should improve retention for the baseline as well as our proposed method. Community nodes can be added at a later point through a periodic job.

\subsection{Method Enhancements}

The Zep baseline provides a solid foundation on which we plan to improve our approach. Additionally, we do not want to feed every user input directly into the knowledge graph, but the extraction reasoning layer which decides what information of the input is interesting to add for the knowledge graph and which information we would rather want to put into a vector database for semantic search.

For retrieval, we provide the tools also included in Zep, and additionally vector database search tools. The agent should also decide in retrieval which tool to use for which purpose. This is enforced through prompt engineering.

In combination, this approach should enable us to have a more fine-grained storage and retrieval mechanism. Consisting of augmentations (selective storage) and retrievals (selective retrieval). This is then tested using the LongMemEval benchmark, for both baselines and our proposed method.

For now, we leave the exact implementation details of the extraction reasoning layer open, as we first want to establish the baselines and then experiment with different approaches for the extraction reasoning layer. One of the ideas that we have for improving the extraction reasoning layer is to create multiple questions about the user input, when then is evaluated in how likely the missing bit of information is to be found with a relation to an entity in a knowledge graph, or through semantic similarity search in a vector database. We illustrate this in Figure \ref{fig:extraction-and-augmentation-architecture-overview}.

\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{img/extraction-and-augmentation-architecture-overview.drawio.png}}
 \caption{Extraction and Augmentation Architecture Overview}
\label{fig:extraction-and-augmentation-architecture-overview}
\end{center}
\vskip -0.2in
\end{figure}

\section{Current Progress}
Summarize what has been accomplished so far.  
Include any preliminary results, experiments, or theoretical findings.  

\subsection{??? (To be determined)}
\subsection{LongMemEval Integration}

We integrated the \textbf{LongMemEval} benchmark \cite{longmemeval2024} to standardize the assessment of long-term memory in multi-session dialogue. 
To make this seamless within our project, we released an open-source wrapper, \texttt{LongMemEval-Advanced-Deeplearning}\footnote{\url{https://github.com/kinsingo/LongMemEval-Advanced-Deeplearning}}, which streamlines dataset ingestion, prediction formatting, and scoring in a reproducible way.

Because the target LLM has not yet been finalized, the current pipeline defaults to the OpenAI API(gpt-4o) as the inference backend, enabling rapid prototyping without local deployment. 
The interface is deliberately model-agnostic: inference calls are abstracted behind a thin layer so we can later substitute locally hosted models or alternative providers with minimal changes. 
Once the project’s model choice is fixed, we will adapt this interface to unify logging and metric visualization with our broader evaluation workflow, while preserving compatibility with LongMemEval’s task structure.



\section{Next Steps}
List your planned future tasks and timeline.  
Identify challenges, resources needed, or potential improvements.  
You may divide this section into short-term and long-term goals.

\subsection{Short-Term Goals}
\begin{itemize}
    \item Implement and test updated model parameters.
    \item Run additional experiments on larger datasets.
\end{itemize}

\subsection{Long-Term Goals}
\begin{itemize}
    \item Extend the method to real-world applications.
    \item Prepare the paper for submission to [Conference/Journal].
\end{itemize}

% \bibliography{references}
\bibliography{paper}
\bibliographystyle{icml2025}

\end{document}
