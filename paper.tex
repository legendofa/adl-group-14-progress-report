%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Memory-Augmented Agentic LLM Using Knowledge Graphs and Vector 
Databases for Context-Aware Reasoning)}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Kieslinger, Julian Felix 2025-82736}{yyy}
\icmlauthor{Salah-Eddine Nifa 2025-82914}{yyy}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Department of Data Science, Seoul National University, Seoul, SK}

\icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
This document serves as a progress report for our project titled Memory-Augmented Agentic LLM Using Knowledge Graphs and Vector Databases for Context-Aware Reasoning.” 
It summarizes the problem being addressed, related work in the area, methodology developed so far, 
current progress, and planned next steps.
\end{abstract}

\section{Problem Definition}
Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating natural language. However, their effectiveness is inherently limited by the fixed-size context window, which restricts their ability to utilize long-term, evolving knowledge during extended interactions. Traditional LLMs lack persistent memory mechanisms to store, retrieve, and update information accumulated over time, leading to challenges in maintaining contextual consistency, adapting to corrections, and providing coherent responses over long-term dialogues or complex tasks.

Current approaches to augment LLMs with memory primarily rely either on unstructured retrieval from static document stores or on isolated vector embeddings, both of which suffer from limited interpretability and insufficient integration between structured and unstructured knowledge. This insufficiency hampers the model’s ability to perform context-aware reasoning, dynamic knowledge updating, and sophisticated planning, particularly in scenarios demanding continuous learning and adaptation.

The core problem addressed in this work is the design of a hybrid memory-augmented, agentic LLM architecture capable of persistent knowledge accumulation, dynamic refinement, and contextually relevant reasoning across long-term interactions. This system must effectively integrate structured memory—using knowledge graphs to represent explicit entities and their relationships—with unstructured semantic memory—using vector databases to enable flexible, similarity-based retrieval. The agent must also autonomously update its internal world model, incorporating user feedback and evolving contextual cues, thereby bridging the gap between limited short-term language context and the necessity for a robust, constantly evolving long-term memory.

Moreover, such an architecture must enable efficient scaling to real-world application complexities, balancing computational resource constraints with the need for accurate and contextually grounded reasoning. To quantitatively evaluate the system’s capabilities, benchmarking against comprehensive datasets such as LongMemEval will provide transparent, verifiable results demonstrating improvements in memory consistency, relevance, and adaptability.

By addressing these challenges, the proposed framework aims to push the boundaries of current LLM capabilities, advancing toward fully autonomous agents capable of sustained, context-aware reasoning and decision-making in diverse real-world domains such as software development assistance, personal knowledge management, and interactive dialogue systems.

\section{Related Works}
Summarize key prior research relevant to your problem.  
Highlight the main methods, results, or ideas that influence your approach.  
Discuss the limitations of existing works that your project aims to overcome.

Graphiti.

Vector Databases in general + pgvector.

Other papers from our reading list.

Simpler approaches like OpenAIs memory implementation: https://medium.com/agentman/building-chatgpt-like-memory-openais-new-feature-and-how-to-create-your-own-3e8e3594b670

\section{Methodology}
Describe your proposed approach in detail.  
Explain your model, algorithm, or theoretical framework.  
If applicable, include equations, diagrams, or pseudocode that clarify your method.

\section{Current Progress}
Summarize what has been accomplished so far.  
Include any preliminary results, experiments, or theoretical findings.  
Use tables or figures as needed.

\begin{table}[ht]
    \caption{Preliminary experimental results.}
    \label{tab:results}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{sc}
    \begin{tabular}{lcc}
    \toprule
    Experiment & Metric & Result \\
    \midrule
    Exp. 1 & Accuracy & 0.87 \\
    Exp. 2 & RMSE & 1.23 \\
    \bottomrule
    \end{tabular}
    \end{sc}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table}

\section{Next Steps}
List your planned future tasks and timeline.  
Identify challenges, resources needed, or potential improvements.  
You may divide this section into short-term and long-term goals.

\subsection{Short-Term Goals}
\begin{itemize}
    \item Implement and test updated model parameters.
    \item Run additional experiments on larger datasets.
\end{itemize}

\subsection{Long-Term Goals}
\begin{itemize}
    \item Extend the method to real-world applications.
    \item Prepare the paper for submission to [Conference/Journal].
\end{itemize}

\bibliography{references}
\bibliographystyle{icml2025}

\end{document}
